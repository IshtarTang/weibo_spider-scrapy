日志2为新版微博记录

2022.10.16
在写爬新版微博页面的程序，还不能用，明天尽量把评论和失败重试给写了

2022.10.18
新版程序能跑了，不过目前的状态算是强行拼凑起来的，除了我之外任何人动都会炸
要给它写个判断重复次数的中间件才行

2022.11.14
旧版页面要被停用了，旧版的爬虫会停止更新，新版的慢慢写

2022.11.30
调整了下结构
想用custom_settings设日志但是没效果，可能只能配自己的组件？等把新版pipline写出来再试试
写了新的微相册下图片程序。不知道微相册以后会不会停用，要是停用就只能再写新的了，还没搞明白新版微博的大图加载机制

2022.12.16（旧版共用）
修改了pipline结构：聚合部分代码单独拿出来写了个文件，可以单独运行；删除simple_wb_info.json相关内容。

2022.12.18
微博旧版切换按钮没了

2022.12.23
可以正常处理转发微博了
如果今天状态好就把评论数量限制写出来，在考虑要不要写成中间件

2022.12.26
新版微博请求正文/评论数据有个{ok:1}表示数据正常，之前是在spider里一个一个判断现在写成中间件了：MyCountDownloaderMiddleware
在meta里加{"my_count": 0}启动这个判断

暂时把获取评论关掉了。因为没写判断评论数量的代码，打开的话只能一直爬完全部

2023.02.20
收到报错：预写文件为空时重启动报错，我这边跑是可以处理空文件的，怪。把if提前了一点，空文件直接跳过。

2023.03.28
文件save_img_new，微相册链接存图片
文件to_vis_new  结果文件转换成可视化程序用的文件（可视化程序没公开）

2023.04.06
get_rcomm() 和 get_ccomm() 二合一 成 get_comm()
加了debug功能：页数限制

2023.04.07
没办法搞成用指令指定配置文件，指令对spider生效，但是setting和pipline没办法从指令读路径，这两又都需要用配置文件
写了run_backup，就是把一堆配置文件的设定依次爬一遍，配合写了批量改配置文件cookies的(感觉有些拉跨,但是能用)

2023.04.09
评论数量指定终于生出来了
获取评论本身还有bug（目前确定无法解决，见2023.04.10日志）

2023.04.10
有些评论点赞数无法获取不打算详细处理了，直接设成-1
部分子评论在pc端确定无法获取，详见结尾的问题记录
git弄不了空文件夹，给log文件夹里加了个文件

2023.4.11
部分根评论无法完全获取，详见结尾的问题记录
给requirements.txt里的两个指定了版本

2023.05.06
问题记录：评论限制失效(已修)

2023.06.17
修复bug：即使设为0也会获取一次子评论的bug
修复bug：评论数量限定重复计数

2023.06.24
问题记录： 判断长微博部分，要找一下key if text_len >= 240:


2024.03.10
添加配置项get_rwb_detail，设为0为不获取源微博详细数据，只获取转发页能获取到的，设为0则获取详细数据（发一次请求）
处理这种能看见转发但原博不可阅读的：https://weibo.com/6591638928/IpXyxmLuO

2024.03.17
增加配置项time_limit

2024.04.06
4月1号微博往headers和cookies里更新了XSRF-TOKEN所以我也得更新

2024.05.04
新增batch_backup.py

2025.02.14
把time_limit的判断修到了判断本页信息是否有效之后

2025.02.26
pipline生成结果文件一点优化
评论获取一点优化
增加批量运行小工具

2025.02.28
增加批量下图片小工具

2025.02.27
将指定config文件的变量合并到了setting.py，并且现在可以通过指令来指定config文件了
为实现用指令指定config，scrapy改为了2.11版本
修复爬不到child comm的bug

2025.03.03
修了start_requests一些逻辑的顺序

2025.03.04
加了auto2模式
设成了深度优先

2025.03.05
如果redis中有上次未完成的请求，本次运行不会重新开始请求主页

2025.07.21
修了之前其实不能完整获取ccomm、rcomm和ccomm的问题
测试性的往里硬塞了爬收藏功能，完全没有写任何配套，我晚点会修的

2025.07.23
拆分出中间件JsonFieldRetryMiddleware，用于判断json响应中是否包含需要的字段
拆分出中间件LoginStatusMiddleware，用于处理登cookies过期
拆分出中间件ResponseStatusHandlerMiddleware，目前只用于处理请求频繁414状态码
MyCountDownloaderMiddleware改为Ok1RetryMiddleware，检查响应中的json数据中是否包含{"ok":1}

2025.07.25
优化parse_wb返回内容结构
parse_wb_simple移动到parsers.py
拆分出 fetcher.build_firse_comment_request

2025.07.26
get_comm合并到parse_comm
从 parse_comm 拆分出 parsers.extract_comm_item
从 parse_comm 拆分出 fetcher.build_comment_turning_request
拆分start，各功能封装到 utils/init_utils
把 comm_tool 和各处写的零碎工具函数整合到 utils 下的 config_utils、file_utils和time_utils

2025.07.29
不记得具体拆分过程了，总之就是拆了
fetcher.py 构建请求
parses.py 解析
skip_rules.py 各种跳过的条件
utils/ 小工具
callbacks.py 想把一些能通用的回调函数拆出来，拆完发现redis的调度器不支持这么用
callback_templates.py 上面那个的替代方案，里面的函数可以很方便的直接装到别的spider里

2025.07.30
parsers.parse_wb参数优化
failure_with_max_id改为只判断root评论

parsers.parse_wb参数优化，failure_with_max_id改为只判断root评论

有兴致的话要写的优化
评论翻页的failure_with_max_id弄成中间件，加开关
build_firse_rcomment_request前面两个过滤条件拆到skip_rules
时间很够的话把config弄成个类



问题记录（微博的问题，不是我的）
部分微博根评论无法完全获取，根评论在获取到一定数量后会开始有一个很迷的逻辑，会返回有max_id但没有data的数据，要继续用新的max_id请求到第15次才会返回有内容的（太离谱了我感觉我以后自己看到都不信所以截了图在留档里），而且继续往后翻一些之后data会一直为空
部分微博（比如这个2803301701/Ir8yTbmQu）的子评论没办法完整获取，翻一些之后data会一直为空

